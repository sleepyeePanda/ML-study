{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 출력층 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망은 분류와 회귀 모두에 이용할 수 있으나, 둘 중 어느 문제냐에 따라 **출력층에서 사용하는 활성화 함수**가 달라짐.  \n",
    "일반적으로 **회귀**에는 **항등 함수**, **분류**에는 **소프트맥스 함수**를 사용.\n",
    "\n",
    "기계학습의 지도학습 문제는 분류와 회귀로 나뉨.  \n",
    "* **분류(classification)** : 데이터가 어느 클래스에 속하느냐는 문제\n",
    "* **회귀(regression)** : 입력 데이터에서 (연속적인) 수치를 예측하는 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 출력층 설계하기\n",
    "\n",
    "* **항등 함수** : 입력을 그대로 출력하여 입력과 출력이 항상 같음.\n",
    "* **소프트맥스 함수** : $y_{k} = \\frac{exp(a_{k})}{\\sum_{i=1}^{n}exp(a_{i})}$  \n",
    "exp(x)는 $e^{x}$을 뜻하는 지수 함수(e는 자연상수),  \n",
    "n은 출력층의 뉴런 수,  \n",
    "$y_{k}$는 그중 k번째 출력임을 뜻함.  \n",
    "**소프트맥스 함수의 분자는 입력신호 $a_{k}$의 지수 함수, 분모는 모든 입력 신호의 지수 함수의 합으로 구성됨.**  \n",
    "소프트맥스 함수의 출력은 모든 입력 신호로 부터 화살표를 받는데 분모에서 보듯 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받기 때문임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **지수 함수(exponential function)** : 거듭제곱의 지수를 변수로 하고, 정의역을 실수 전체로 정의하는 초월 함수. 로그 함수의 역함수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.34985881, 18.17414537, 54.59815003])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0.3, 2.9, 4.0])\n",
    "# 지수함수\n",
    "exp_a = np.exp(a)\n",
    "exp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.1221542101633"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_exp_a = np.sum(exp_a)\n",
    "sum_exp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01821127, 0.24519181, 0.73659691])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = exp_a / sum_exp_a\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous softmax\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01821127, 0.24519181, 0.73659691])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax ([0.3, 2.9, 4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([999,1000,1001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 함수 구현 시 주의점\n",
    "소프트맥스 함수는 지수 함수를 사용하므로 컴퓨터로 계산할 때는 오버플로 결함이 있음.  \n",
    "그리고 지수함수의 결과값이 매우 큰 값끼리 나눗셈을 하면 결과 수치가 **불안정**해짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 함수의 개선\n",
    "$$y_{k} = \\frac{exp(a_{k})}{\\sum_{i=1}^{n}exp(a_{i})} = \n",
    "\\frac{Cexp(a_{k})}{C\\sum_{i=1}^{n}exp(a_{i})} = \n",
    "\\frac{exp(a_{k} + logC)}{\\sum_{i=1}^{n}exp(a_{i}+logC)} =\n",
    "\\frac{exp(a_{k}+C^{'})}{\\sum_{i=1}^{n}exp(a_{i}+C^{'})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revised softmax\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c) # 오버플로 대책\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([999,1000,1001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 함수의 특징\n",
    "* 소프트맥스 함수의 출력은 0에서 1.0사이의 실수  \n",
    "* 소프트맥스 함수 출력의 총합은 1(이 성질로 인해 소프트맥스 함수의 출력을 **확률**로 해석, 문제를 확률적(통계적)으로 대응 가능. 신경망을 이용한 **분류**에서는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스를 인식함.)\n",
    "* 지수 함수 y = exp(x)가 단조 증가 함수이므로 소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않음.\n",
    "\n",
    "* 소프트맥스 함수를 적용해도 출력이 가장 큰 뉴런의 위치는 달라지지 않음. 결과적으로 신경망으로 분류할 때는 출력층의 소프트맥스를 생략하고, 지수 함수 계산에 드는 자원 낭비를 줄이기 위해서도 **생략하는 것이 일반적**.\n",
    "\n",
    "* **단조 함수(monotonic function)** : 주어진 순서를 보존하는 함수. $a, b$가 $a\\leq b$일 때 $f(a) \\leq f(b)$ 성립"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계학습의 문제 풀이는 학습과 추론(inference)의 두 단계를 거쳐 이루어짐. 학습 단계에서 훈련 데이터를 사용해 가중치 매개변수(모델)를 학습하고, 추론 단계에서 앞서 학습한 매개변수(모델)로 미지의 데이터에 대해서 추론(분류)함. 추론 과정을 **신경망의 순전파(forward propagation)**이라고도함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 출력층의 뉴런 수 정하기\n",
    "출력층의 뉴런 수는 풀고자 하는 문제에 따라 적절히 정해야 함.(분류에서는 분류하고 싶은 클래스 수로 설정하는 것이 일반적.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 손글씨 숫자 인식\n",
    "* **MNIST datasbase(Modified National Institute of Standards and Technology database)** : large database of handwritten digits that is commonly used for training various image processing systems. 28\\*28 크기의 회색조 이미지(1채널)이며, 각 픽셀은 0에서 255까지의 값을 취함.(RGB 3채널, RGBA 4채널)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from dataset.mnist import load_mnist\n",
    "# 전처리 : 신경망의 입력 데이터에 특정 변환을 가하는 것(백색화(whitening) : 전체 데이터를 균일하게 분포시킴)\n",
    "# flatten=True : 입력 이미지를 1차원으로 만듦.\n",
    "# normalize=True : 입력 이미지의 픽셀을 0.0~1.0 사이의 값으로 정규화함.\n",
    "# one_hot_label=True : 원-핫 인코딩 형태로 변환함.\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten = True, normalize = False)\n",
    "\n",
    "print(x_train.shape) # 훈련 이미지\n",
    "print(t_train.shape) # 훈련 레이블\n",
    "print(x_test.shape) # 시험 이미지\n",
    "print(t_test.shape) # 시험 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img)) # 넘파이로 저장된 이미지를 PIL용 데이터 객체로 변환함.\n",
    "    pil_img.show()\n",
    "    \n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten = True, normalize = False)\n",
    "\n",
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "print(label)\n",
    "\n",
    "print(img.shape)\n",
    "img = img.reshape(28,28)\n",
    "print(img.shape)\n",
    "img_show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(flatten = True, normalize = False, one_hot_label= False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"sample_weight.pkl\", 'rb') as f: # 이진 파일(바이너리 파일)을 열어서 1바이트씩 읽어, 화면에 16진으로 출력\n",
    "        network = pickle.load(f)\n",
    "        \n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(a1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "accuracy_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y) # np.argmax(x) : x(array_like) 원소 중 가장 큰 값의 latten array형태일 때의 인덱스를 반환\n",
    "    if p == t[i]:\n",
    "        accuracy_cnt += 1\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf> np.argmax(a, axis) 다차원일 경우\n",
    "http://gomguard.tistory.com/145"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **전처리(pre-processing)** : 신경망의 입력데이터에 특정 변환을 가하는 것.식별 능력을 개선하고 학습 속도 향상을 위해 사용함.  \n",
    "    \\- **정규화(nomalization)** : 데이터를 특정 범위로 변환하는 처리. 데이터의 전체 평균과 표준편차를 이용하여 데이터들이 0을 중심으로 분포하도록 이동하거나 데이터의 확산 범위를 제한함.  \n",
    "    \\- **백색화(whitening)** : 전체 데이처를 균일하게 분포시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 배치 처리\n",
    "* **배치(batch)** : 하나로 묶은 입력 데이터\n",
    "### 배치 처리의 장점\n",
    "이미지 1장당 처리 시간을 줄여줌. 수치 계산 라이브러리 대부분이 큰 배열을 효율적으로 처리할 수 있도록 고도로 최적화되어 있고, 커다란 신경망에서 데이터 전송이 병목으로 작용하는 경우 버스에 주는 부하를 줄여주기 때문.(느린 I/O를 통해 데이터를 읽는 횟수가 줄어, 빠른 CPU나 GPU로 순수 계산을 수행하는 비율이 높아짐.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(0, len(x), batch_size): # range(start, end, step)\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis = 1) # agrmax([0.1, 0.2, 0.3], sxis = 1) == 2\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "    \n",
    "print('Accuracy:'+str(float(accuracy_cnt)/len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
